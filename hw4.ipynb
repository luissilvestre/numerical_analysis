{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc05347b",
   "metadata": {},
   "source": [
    "# Homework 4\n",
    "\n",
    "Solve exercises 10.2, 10.15, 10.19, 11.15\n",
    "\n",
    "In the next cell, we copy all the functions from the last homework. We will use them for comparison with the new ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bd1234",
   "metadata": {},
   "outputs": [],
   "source": [
    "function LU(A)\n",
    "    n, m = size(A) # A is supposed to be a square matrix, so hopefully n and m will be equal.\n",
    "    \n",
    "    # We initalize L with zeros and U to be the same as A.\n",
    "    L = zeros(n,m)\n",
    "    U = copy(A)\n",
    "    \n",
    "    for k in 1:n\n",
    "        L[k,k] = 1\n",
    "        for i in (k+1):n\n",
    "            L[i,k] = U[i,k]/U[k,k]\n",
    "            U[i,:] = U[i,:] - L[i,k]*U[k,:]\n",
    "        end\n",
    "    end\n",
    "    return L, U\n",
    "end\n",
    "\n",
    "function backward_substitute(U,y)\n",
    "    n, m = size(U)\n",
    "    r, = size(y)\n",
    "    @assert n == m == r\n",
    "    x = zeros(n)\n",
    "    \n",
    "    for i in n:-1:1\n",
    "        tail = 0\n",
    "        for j in i+1:n\n",
    "            tail = tail + U[i,j]*x[j]\n",
    "        end\n",
    "        x[i] = (y[i] - tail)/U[i,i]\n",
    "    end\n",
    "    \n",
    "    return x\n",
    "end\n",
    "\n",
    "function forward_substitute(L,y)\n",
    "    n, m = size(L)\n",
    "    r, = size(y)\n",
    "    @assert n == m == r\n",
    "    x = zeros(n)\n",
    "    for i in 1:n\n",
    "        tail = 0\n",
    "        for j in 1:i-1\n",
    "            tail = tail + L[i,j]*x[j]\n",
    "        end\n",
    "        x[i] = (y[i] - tail)/L[i,i]\n",
    "    end\n",
    "    return x\n",
    "end\n",
    "\n",
    "function solve(A,f)\n",
    "    n, m = size(A)\n",
    "    r, = size(f)\n",
    "    @assert n == m == r\n",
    "    \n",
    "    # Something goes here\n",
    "    L, U = LU(A)\n",
    "    y = forward_substitute(L,f)\n",
    "    x = backward_substitute(U,y)\n",
    "    \n",
    "    return(x)\n",
    "end\n",
    "\n",
    "function big_matrix(n)\n",
    "    A = zeros(n^2,n^2)\n",
    "    for i in 1:n^2\n",
    "        A[i,i] = 4\n",
    "        if mod(i,n)!=0\n",
    "            A[i+1,i] = -1\n",
    "            A[i,i+1] = -1\n",
    "        end\n",
    "        if i<=n^2-n\n",
    "            A[i,i+n] = -1\n",
    "            A[i+n,i] = -1\n",
    "        end\n",
    "    end\n",
    "    return A\n",
    "end\n",
    "\n",
    "function big_rhs(n, f)\n",
    "    y = zeros(n^2)\n",
    "    for i in 1:n\n",
    "        y[i] += f(i/(n+1),0.)\n",
    "        y[n^2-n+i] += f(i/(n+1),1.)\n",
    "    end\n",
    "        \n",
    "    for i in 1:n\n",
    "        y[n*i] += f(1.,i/(n+1))\n",
    "        y[n*i-n+1] += f(0.,i/(n+1))\n",
    "    end\n",
    "    return y\n",
    "end\n",
    "\n",
    "function T(u::Array{<:Real,2})\n",
    "    n, m = size(u)\n",
    "    v = similar(u) # This line creates another array with the same dimensions and type as u\n",
    "    for i in 1:n\n",
    "        for j in 1:m\n",
    "            v[i,j] = 4*u[i,j]\n",
    "            if i>1 v[i,j] -= u[i-1,j] end\n",
    "            if i<n v[i,j] -= u[i+1,j] end\n",
    "            if j>1 v[i,j] -= u[i,j-1] end\n",
    "            if j<m v[i,j] -= u[i,j+1] end\n",
    "        end\n",
    "    end\n",
    "    return v\n",
    "end\n",
    "\n",
    "function matrix_rhs(n::Integer, f::Function)\n",
    "    b = zeros(n,n)\n",
    "    for i in 1:n\n",
    "        b[1,i] += f(0.,i/(n+1))\n",
    "        b[n,i] += f(1.,i/(n+1))\n",
    "        b[i,1] += f(i/(n+1),0.)\n",
    "        b[i,n] += f(i/(n+1),1.)\n",
    "    end\n",
    "    return b\n",
    "end  \n",
    "\n",
    "# The following function is supposed to be equivalent as doing forward substitution with the lower-triangular \n",
    "# part of the matrix. Since the matrix is sparce, it is much more efficient to implement it as a function than \n",
    "# to write extract the lower-triangular part of A and called forward_substitute from above.\n",
    "\n",
    "function forward_substitute_lower_diagonal_part(v::Array{<:Real,2})\n",
    "    n, m = size(v)\n",
    "    u = zeros(n,m)\n",
    "    for i in 1:n\n",
    "        for j in 1:m\n",
    "            s = v[i,j]\n",
    "            if i>1 s += u[i-1,j] end\n",
    "            if j>1 s += u[i,j-1] end\n",
    "            u[i,j] = 0.25 * s\n",
    "        end\n",
    "    end\n",
    "    return u\n",
    "end\n",
    "\n",
    "# The following function corresponds to applying the upper triangular part of A to u.\n",
    "function upper_triangular_part(u::Array{<:Real,2})\n",
    "    n, m = size(u)\n",
    "    v = zeros(n,m)\n",
    "    for i in 1:n\n",
    "        for j in 1:m\n",
    "            if i<n v[i,j] -= u[i+1,j] end\n",
    "            if j<m v[i,j] -= u[i,j+1] end\n",
    "        end\n",
    "    end\n",
    "    return v\n",
    "end\n",
    "\n",
    "function gauss_seidel(u::Array{<:Real,2}, b::Array{<:Real,2}, iterations::Integer)\n",
    "    for i in 1:iterations\n",
    "        u = forward_substitute_lower_diagonal_part(b - upper_triangular_part(u))\n",
    "    end\n",
    "    return u\n",
    "end    \n",
    "\n",
    "function inner_product(u::Array{<:Real,2}, v::Array{<:Real,2})\n",
    "    s = 0\n",
    "    n,m = size(u)\n",
    "    for i in 1:n\n",
    "        for j in 1:m\n",
    "            s += u[i,j] * v[i,j]\n",
    "        end\n",
    "    end\n",
    "    return s\n",
    "end\n",
    "\n",
    "function gradient_descent(u::Array{<:Real,2}, b::Array{<:Real,2}, iterations::Integer)\n",
    "    for i in 1:iterations\n",
    "        r = T(u) - b\n",
    "        a = inner_product(r,r) / inner_product(T(r),r)\n",
    "        u = u - a*r\n",
    "    end\n",
    "    return u\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a742212",
   "metadata": {},
   "source": [
    "Now you are supposed to implement the conjugate gradients method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1080259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function conjugate_gradients(u::Array{<:Real,2}, b::Array{<:Real,2}, iterations::Integer)\n",
    "    # ...\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb900c5",
   "metadata": {},
   "source": [
    "Let us test how long these algorithms take in practice. So, we will time their execution time with a 40x40 mesh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357a7300",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 40\n",
    "f(x,y) = 1 - sin(pi*x)\n",
    "partition = 1/(n+1) : 1/(n+1) : 1-1/(n+1)\n",
    "\n",
    "# We use the following function to roughly measure the size of vectors or matrices.\n",
    "function supnorm(u)\n",
    "    s = 0\n",
    "    for entry in u\n",
    "        s = max(abs(entry),s)\n",
    "    end\n",
    "    return s\n",
    "end\n",
    "\n",
    "# Using Gaussian elimination\n",
    "y = big_rhs(n,f)\n",
    "A = big_matrix(n)\n",
    "@time x = solve(A,y)\n",
    "println(\"Accuracy of Gaussian elimination: \", supnorm(A*x-y) / n^2)\n",
    "\n",
    "# Using Gauss-Seidel iteration\n",
    "# We can test different number of iterations\n",
    "\n",
    "iterations = div(n^2,10)\n",
    "b = matrix_rhs(n,f)\n",
    "u = zeros(n,n)\n",
    "@time u = gauss_seidel(u,b,iterations)\n",
    "println(\"Accuracy of Gauss Seidel with \", iterations, \" iterations: \", supnorm(T(u)-b)/n^2)\n",
    "\n",
    "iterations = n^2\n",
    "b = matrix_rhs(n,f)\n",
    "u = zeros(n,n)\n",
    "@time u = gauss_seidel(u,b,iterations)\n",
    "println(\"Accuracy of Gauss Seidel with \", iterations, \" iterations: \", supnorm(T(u)-b)/n^2)\n",
    "\n",
    "iterations = 10*n^2\n",
    "b = matrix_rhs(n,f)\n",
    "u = zeros(n,n)\n",
    "@time u = gauss_seidel(u,b,iterations)\n",
    "println(\"Accuracy of Gauss Seidel with \", iterations, \" iterations: \", supnorm(T(u)-b)/n^2)\n",
    "\n",
    "# Using Gradient descent\n",
    "# We can test different number of iterations\n",
    "\n",
    "iterations = n^2\n",
    "b = matrix_rhs(n,f)\n",
    "u = zeros(n,n)\n",
    "@time u = gradient_descent(u,b,iterations)\n",
    "println(\"Accuracy of Gradient descent with \", iterations, \" iterations: \", supnorm(T(u)-b)/n^2)\n",
    "\n",
    "# Using Conjugate radients\n",
    "# We can test different number of iterations\n",
    "\n",
    "iterations = div(n^2,10)\n",
    "b = matrix_rhs(n,f)\n",
    "u = zeros(n,n)\n",
    "@time u = conjugate_gradients(u,b,iterations)\n",
    "println(\"Accuracy of conjugate gradients with \", iterations, \" iterations: \", supnorm(T(u)-b)/n^2)\n",
    "\n",
    "iterations = n^2\n",
    "b = matrix_rhs(n,f)\n",
    "u = zeros(n,n)\n",
    "@time u = conjugate_gradients(u,b,iterations)\n",
    "println(\"Accuracy of conjugate gradients with \", iterations, \" iterations: \", supnorm(T(u)-b)/n^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb1e581",
   "metadata": {},
   "source": [
    "Conjugate gradients wins by a large margin. In this example, we are getting the exact solution in even fewer iterations than expected. I don't have a good explanation of why, and we shouldn't expect that in general.\n",
    "\n",
    "We can even solve the problem with a massive mesh of 200x200. It takes less than a minute to get the exact solution in my (old) office computer. Good luck doing that with any of the other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10174e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200\n",
    "f(x,y) = 1 - sin(pi*x)\n",
    "partition = 1/(n+1) : 1/(n+1) : 1-1/(n+1)\n",
    "iterations = n^2\n",
    "b = matrix_rhs(n,f)\n",
    "u = zeros(n,n)\n",
    "@time u = conjugate_gradients(u,b,iterations)\n",
    "println(\"Accuracy of conjugate gradients with \", iterations, \" iterations: \", supnorm(T(u)-b)/n^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202c33fe",
   "metadata": {},
   "source": [
    "## Final fun movie\n",
    "\n",
    "Let us make a movie to compare the speed of gradient descent with conjugate gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f6e55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "iterations = 5\n",
    "n = 40\n",
    "partition = 1/(n+1) : 1/(n+1) : 1-1/(n+1)\n",
    "\n",
    "let b = matrix_rhs(n,f), u = zeros(n,n), v = zeros(n,n)\n",
    "    @gif for i in 1:300\n",
    "        u = gradient_descent(u,b,iterations)\n",
    "        v = conjugate_gradients(v,b,iterations)\n",
    "        sgd1 = surface(partition,partition,u, title=string(i*iterations)*\" iterations\")\n",
    "        sgd2 = surface(partition,partition,v, title=string(i*iterations)*\" iterations\")\n",
    "        sg = plot(sgd1,sgd2,layout=2, size=(800,300))\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aa5f30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.3",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
